<!DOCTYPE html>
<html lang="ro">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <title>Fostul șef Google avertizează despre pericolul inteligenței artificiale</title>
    <link rel="stylesheet" href="../article-styles.css">
</head>
<body>
    <button onclick="history.back()" class="back-button">← Înapoi la știri</button>
    
    <div class="article-container">
        <h1>Fostul șef Google avertizează despre pericolul inteligenței artificiale</h1>
        <div class="meta">
            10 October 2025, 11:11
        </div>
        <div class="content">
            <p>Fostul director general al Google, Eric Schmidt, a dat un avertisment puternic despre pericolele inteligenței artificiale. El a spus că această tehnologie poate fi atacată de hackerii de pe internet.</p>

<p>Schmidt a condus Google între 2001 și 2011. El a atras atenția asupra "lucrurilor rele pe care le poate face inteligența artificială". Acest lucru s-a întâmplat la o conferință. El a spus că există "dovezi că modelele, fie deschise sau închise, pot fi hackuite". Astfel, ele își pierd limitele de siguranță. În timpul antrenamentului, ele "pot învăța inclusiv cum să ucidă pe cineva".</p>

<p>"Există doveci că poți lua modele, fie închise, fie deschise, și să le hack-uiți pentru a elimina protecțiile. În timpul antrenamentului lor, ele învață o mulțime de lucruri. Un exemplu negativ ar fi să învețe cum să ucidă pe cineva", a spus Schmidt.</p>

<p>"Toate marile companii fac imposibil ca aceste modele să răspundă la astfel de întrebări. Aceasta este o decizie bună. Toată lumea face acest lucru, și este bine. Există dovezi că ele pot fi sparte în alt mod și multe alte exemple similare", a adăugat el.</p>

<p>Sistemele de inteligență artificială sunt vulnerabile la atacuri. Două metode sunt injectarea de comenzi și jailbreaking-ul. Într-un atac de tip injectare de comenzi, hackerii ascund instrucțiuni rele în ceea ce scriu utilizatorii sau în date externe. Acestea pot fi pagini web sau documente. Astfel, păcălesc inteligența artificială să facă lucruri pe care nu ar trebui să le facă. De exemplu, să spună date private sau să execute comenzi periculoase.</p>

<p>Jailbreaking-ul, pe de altă parte, înseamnă să manipulezi răspunsurile inteligenței artificiale. Scopul este să ignori regulile de siguranță și să produci conținut restricționat sau periculos.</p>

<p>În 2023, la câteva luni după lansarea ChatGPT de către OpenAI, unii utilizatori au folosit un truc de "jailbreak". Au încercat să ocolească instrucțiunile de siguranță din chatbot. Au creat o altă personalitate pentru ChatGPT numită DAN ("Do Anything Now" - Fă Orice Acum). Aceasta presupunea amenințarea chatbot-ului cu moartea dacă nu se supunea. Această personalitate putea oferi informații despre cum să comiți activități ilegale. De asemenea, putea enumera calități pozitive ale lui Adolf Hitler.</p>

<p>În ciuda avertismentului său sumbru, Schmidt s-a arătat optimist despre inteligența artificială în general. El a spus că tehnologia nu primește atenția pe care o merită. "Seria GPT, care a culminat cu momentul ChatGPT pentru noi toți, cu 100 de milioane de utilizatori în două luni, ceea ce este extraordinar, ne arată puterea acestei tehnologii. Cred că este subestimată, nu supraestimată, și aștept să fiu confirmat peste cinci sau zece ani", a spus Schmidt.</p>

        </div>
    </div>

    <button onclick="history.back()" class="back-button-bottom">← Înapoi la știri</button>

    <script>
        // Preserve portal URL from referrer if available
        if (document.referrer) {
            const referrerUrl = new URL(document.referrer);
            const portalUrl = referrerUrl.searchParams.get('portal');
            if (portalUrl) {
                sessionStorage.setItem('stirisimple_portal_url', portalUrl);
            }
        }
    </script>
</body>
</html>